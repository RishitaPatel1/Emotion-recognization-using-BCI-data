{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a1794b-7d48-4b83-8ba8-18a07cf147cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# === Data Loading and Preprocessing ===\n",
    "def load_and_preprocess_data(base_path='D:/DL_Project/', file_ids=range(1, 20)):\n",
    "    all_eeg_data, all_labels = [], []\n",
    "    for fid in file_ids:\n",
    "        file_num = str(fid)\n",
    "        preprocessed_file = f'{base_path}eeg_preprocessed/{file_num}.mat'\n",
    "        features_file = f'{base_path}eeg_features/{file_num}.mat'\n",
    "        labels_file = f'{base_path}continuous_labels/{file_num}.mat'\n",
    "\n",
    "        # Load .mat files\n",
    "        preprocessed_data = scipy.io.loadmat(preprocessed_file)\n",
    "        features_data = scipy.io.loadmat(features_file)\n",
    "        labels_data = scipy.io.loadmat(labels_file)\n",
    "\n",
    "        print(f\"Processing file set {file_num}:\")\n",
    "        print(\"Keys in Preprocessed File:\", preprocessed_data.keys())\n",
    "        print(\"Keys in Features File:\", features_data.keys())\n",
    "        print(\"Keys in Labels File:\", labels_data.keys())\n",
    "\n",
    "        # Load EEG features (differential entropy 'de_1' to 'de_80')\n",
    "        eeg_features = []\n",
    "        fixed_time_segments = 32  # Increased from 15 to 32\n",
    "        for key in range(1, 81):\n",
    "            key_str = f'de_{key}'\n",
    "            if key_str in features_data:\n",
    "                trial_data = features_data[key_str]\n",
    "                print(f\"Shape of trial {key} ('{key_str}'): {trial_data.shape}\")\n",
    "\n",
    "                # Ensure shape is (time_segments, bands, channels=62)\n",
    "                if trial_data.shape[2] == 62:\n",
    "                    pass\n",
    "                elif trial_data.shape[0] == 62:\n",
    "                    trial_data = trial_data.transpose(2, 1, 0)\n",
    "                elif trial_data.shape[1] == 62:\n",
    "                    trial_data = trial_data.transpose(2, 0, 1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected shape for trial {key}: {trial_data.shape}\")\n",
    "\n",
    "                # Pad or crop to fixed_time_segments\n",
    "                current_time_segments = trial_data.shape[0]\n",
    "                if current_time_segments > fixed_time_segments:\n",
    "                    trial_data = trial_data[:fixed_time_segments, :, :]\n",
    "                elif current_time_segments < fixed_time_segments:\n",
    "                    pad_width = fixed_time_segments - current_time_segments\n",
    "                    trial_data = np.pad(trial_data, ((0, pad_width), (0, 0), (0, 0)), mode='constant')\n",
    "                \n",
    "                eeg_features.append(trial_data)\n",
    "\n",
    "        if not eeg_features:\n",
    "            raise ValueError(f\"No EEG features found in {features_file}\")\n",
    "        eeg_features = np.array(eeg_features)\n",
    "        print(f\"EEG features shape for file {file_num}: {eeg_features.shape}\")\n",
    "\n",
    "        # Load and process labels\n",
    "        labels = []\n",
    "        for key in range(1, 81):\n",
    "            key_str = str(key)\n",
    "            if key_str in labels_data:\n",
    "                label = labels_data[key_str]\n",
    "                current_time_segments = label.shape[1] if label.ndim > 1 else 1\n",
    "                if current_time_segments > fixed_time_segments:\n",
    "                    label = label[:, :fixed_time_segments]\n",
    "                elif current_time_segments < fixed_time_segments:\n",
    "                    pad_width = fixed_time_segments - current_time_segments\n",
    "                    label = np.pad(label, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                labels.append(label)\n",
    "\n",
    "        if not labels:\n",
    "            raise ValueError(f\"No labels found in {labels_file}\")\n",
    "        labels = np.array(labels)\n",
    "        print(f\"Raw labels shape for file {file_num}: {labels.shape}\")\n",
    "\n",
    "        # Average labels over time segments\n",
    "        if labels.ndim == 3:\n",
    "            labels = np.mean(labels, axis=2)\n",
    "        elif labels.ndim == 2:\n",
    "            labels = np.mean(labels, axis=1, keepdims=True)\n",
    "        labels = labels.squeeze()\n",
    "\n",
    "        # Map continuous labels to discrete emotions\n",
    "        discrete_labels = np.zeros(labels.shape[0], dtype=int)\n",
    "        for i, label in enumerate(labels):\n",
    "            label = label / 100.0 if label > 10 else label / 10.0  # Normalize\n",
    "            if label <= 0.2:\n",
    "                discrete_labels[i] = 0  # Neutral\n",
    "            elif 0.2 < label <= 0.4:\n",
    "                discrete_labels[i] = 1  # Sad\n",
    "            elif 0.4 < label <= 0.6:\n",
    "                discrete_labels[i] = 2  # Happy\n",
    "            elif 0.6 < label <= 0.8:\n",
    "                discrete_labels[i] = 3  # Fear\n",
    "            else:\n",
    "                discrete_labels[i] = 4  # Angry\n",
    "        labels = discrete_labels\n",
    "        print(f\"Processed labels shape for file {file_num}: {labels.shape}, Unique labels: {np.unique(labels)}\")\n",
    "\n",
    "        all_eeg_data.append(eeg_features)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Concatenate across files\n",
    "    all_eeg_data = np.concatenate(all_eeg_data, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    print(f\"Total EEG data shape: {all_eeg_data.shape}, Total labels shape: {all_labels.shape}\")\n",
    "\n",
    "    # Reshape for CNN (32x64)\n",
    "    n_trials = all_eeg_data.shape[0]\n",
    "    eeg_data = []\n",
    "    for trial in range(n_trials):\n",
    "        trial_data = all_eeg_data[trial]\n",
    "        trial_data = np.mean(trial_data, axis=1)  # Average over bands, shape: (time_segments, 62)\n",
    "        # Pad or crop time segments to 32\n",
    "        if trial_data.shape[0] > 32:\n",
    "            trial_data = trial_data[:32, :]\n",
    "        elif trial_data.shape[0] < 32:\n",
    "            trial_data = np.pad(trial_data, ((0, 32 - trial_data.shape[0]), (0, 0)), mode='constant')\n",
    "        # Pad channels to 64\n",
    "        if trial_data.shape[1] < 64:\n",
    "            trial_data = np.pad(trial_data, ((0, 0), (0, 64 - trial_data.shape[1])), mode='constant')\n",
    "        eeg_data.append(trial_data)\n",
    "\n",
    "    eeg_data = np.array(eeg_data)\n",
    "    print(f\"Reshaped EEG data shape: {eeg_data.shape}\")  # Should be (n_trials, 32, 64)\n",
    "\n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    eeg_data_flat = eeg_data.reshape(eeg_data.shape[0], -1)\n",
    "    eeg_data = scaler.fit_transform(eeg_data_flat).reshape(eeg_data.shape[0], 32, 64)\n",
    "\n",
    "    # Balance classes with SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    eeg_data_flat = eeg_data.reshape(eeg_data.shape[0], -1)\n",
    "    eeg_data_resampled, labels_resampled = smote.fit_resample(eeg_data_flat, all_labels)\n",
    "    eeg_data = eeg_data_resampled.reshape(-1, 32, 64)\n",
    "    print(f\"Resampled data shape: {eeg_data.shape}, Labels shape: {labels_resampled.shape}\")\n",
    "    print(f\"Class distribution: {dict(zip(*np.unique(labels_resampled, return_counts=True)))}\")\n",
    "\n",
    "    return eeg_data, labels_resampled\n",
    "\n",
    "# === Improved Hybrid Model ===\n",
    "class ImprovedHybridModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ImprovedHybridModel, self).__init__()\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Output: (32, 16, 32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Output: (64, 8, 16)\n",
    "        # LSTM\n",
    "        self.hidden_size = 256\n",
    "        self.lstm = nn.LSTM(64 * 16, self.hidden_size, num_layers=2, batch_first=True, bidirectional=True, dropout=0.4)\n",
    "        self.attention = Attention(self.hidden_size * 2)\n",
    "        # Fully connected layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))  # (batch, 32, 16, 32)\n",
    "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))  # (batch, 64, 8, 16)\n",
    "        x = x.view(-1, 8, 64 * 16)  # (batch, 8, 1024)\n",
    "        lstm_out, (hn, _) = self.lstm(x)\n",
    "        hn = torch.cat((hn[-2], hn[-1]), dim=1)  # Concatenate bidirectional hidden states\n",
    "        context = self.attention(hn, lstm_out)\n",
    "        out = self.dropout(self.relu(context))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# === Attention Mechanism ===\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / (self.hidden_size ** 0.5)\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size, seq_len, _ = encoder_outputs.size()\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.matmul(self.v)\n",
    "        attn_weights = torch.softmax(energy, dim=1).unsqueeze(2)\n",
    "        context = (attn_weights * encoder_outputs).sum(dim=1)\n",
    "        return context\n",
    "\n",
    "# === Training and Evaluation with Mixup ===\n",
    "def train_and_evaluate(model, train_loader, test_loader, num_epochs=100, device='cuda' if torch.cuda.is_available() else 'cpu', alpha=1.0):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Increased weight decay\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Apply mixup with 50% probability\n",
    "            if np.random.rand() < 0.5:\n",
    "                lam = np.random.beta(alpha, alpha)\n",
    "                index = torch.randperm(inputs.size(0)).to(device)\n",
    "                mixed_inputs = lam * inputs + (1 - lam) * inputs[index]\n",
    "                outputs = model(mixed_inputs)\n",
    "                loss = lam * criterion(outputs, labels) + (1 - lam) * criterion(outputs, labels[index])\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step(epoch + 1)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return train_losses, val_losses, accuracy, precision, recall, f1, cm\n",
    "\n",
    "# === Main Execution ===\n",
    "eeg_data, labels = load_and_preprocess_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(eeg_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare data for CNN and Hybrid\n",
    "X_train_cnn = torch.FloatTensor(X_train).unsqueeze(1)  # Shape: (n_train, 1, 32, 64)\n",
    "X_test_cnn = torch.FloatTensor(X_test).unsqueeze(1)    # Shape: (n_test, 1, 32, 64)\n",
    "y_train_t = torch.LongTensor(y_train)\n",
    "y_test_t = torch.LongTensor(y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset_cnn = TensorDataset(X_train_cnn, y_train_t)\n",
    "test_dataset_cnn = TensorDataset(X_test_cnn, y_test_t)\n",
    "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=32, shuffle=True)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32)\n",
    "\n",
    "# Models (focusing on Hybrid)\n",
    "models = {\n",
    "    'Hybrid': ImprovedHybridModel()\n",
    "}\n",
    "\n",
    "# Training and evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} Model...\")\n",
    "    train_loader = train_loader_cnn\n",
    "    test_loader = test_loader_cnn\n",
    "    train_losses, val_losses, acc, prec, rec, f1, cm = train_and_evaluate(model, train_loader, test_loader, alpha=1.0)\n",
    "    results[name] = {'train_losses': train_losses, 'val_losses': val_losses, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'confusion_matrix': cm}\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# === Visualization ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results['Hybrid']['train_losses'], label='Train Loss')\n",
    "plt.plot(results['Hybrid']['val_losses'], label='Val Loss')\n",
    "plt.title('Hybrid Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
